<!DOCTYPE html>
<html>
	<head>
		<title>Hank Quinlan, Horrible Cop</title>
		<!-- link to main stylesheet -->
		<link rel="stylesheet" type="text/css" href="/css/main.css">
	</head>
	<body>
		<nav>
    		<ul>
        		<li><a href="/">Home</a></li>
	        	<li><a href="/about">About</a></li>
        		<li><a href="/cv">CV</a></li>
        		<li><a href="/blog">Blog</a></li>
    		</ul>
		</nav>
		<div class="container">
    		<div class="blurb">
        		<h1>Hi there, I'm Hank Quinlan!</h1>
				<p>This project is on the generation and evaluation of creativity in the specific domain of music style transformation using deep learning models. As discussed in class, creativity is a broad domain, and computational creativity can be applied to a wide array of fields from paintings, music to mathematical formulas. Creativity can be demonstrated in multiple ways as well. For example, one can build sophisticated story grammars (e.g. MEXICA), train a Markov model, or build a deep learning neural network. In this project, I focus on creativity generated by deep learning neural network models because their immense possibilities with respect to node connections, activation functions, and layer structures are especially fascinating to me, and in my personal view, resemble the most to human creativity. In addition, I am very interested in music generation. However, I do not have a background in music theory or professional music appreciation, so it would be difficult for me to understand and evaluate the quality of music generated from scratch. Thus, I chose the more straightforward area of creating something like the “cover music”, transitioning existing music pieces from one style to another and trying to retain the original content of the music while having a clear contrast in the styles of the original and the generated music pieces. </p>
				<p>Different computer scientists have focused on different ways to transform music styles. Research by Malik and Ek (2017) focused on changing music styles by predicting the note velocities from a sequence of notes by usings models with LSTM layers. The research project by Suraj Jayakumar (2017), on the other hand, gives a broad exploration of how different models, from the baseline seq-2-seq model to the more sophisticated VAE-GAN and the Seq-GAN models, perform on music style transformation. However, I was most interested in the paper by Brunner, Wang, Wattenhofer, and Zhao (2018) with the application of CycleGAN. The CycleGAN model’s structure is very intricate. It contains the generator and discriminator components which are trained together to balance each other’s model less. In addition, the loss function of the generator not only takes into account the quality of the generated music, but also the music piece converted back from the generated music to evaluate how much original content was retained after transformation. In addition, a music style classifier was trained to evaluate the performance of the CycleGAN.</p>				<p><em>A Touch of Evil</em> Don't trust me. <a href="/about">Read more about my life...</a></p>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<footer>
    		<ul>
        		<li><a href="mailto:hankquinlanhub@gmail.com">email</a></li>
        		<li><a href="https://github.com/hankquinlan">github.com/hankquinlan</a></li>
			</ul>
		</footer>
	</body>
</html>
