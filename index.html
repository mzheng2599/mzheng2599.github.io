<!DOCTYPE html>
<html>
	<head>
		<title>Hank Quinlan, Horrible Cop</title>
		<!-- link to main stylesheet -->
		<link rel="stylesheet" type="text/css" href="/css/main.css">
	</head>
	<body>
		<nav>
    		<ul>
        		<li><a href="/">Home</a></li>
	        	<li><a href="/about">About</a></li>
        		<li><a href="/cv">CV</a></li>
        		<li><a href="/blog">Blog</a></li>
    		</ul>
		</nav>
		<div class="container">
    		<div class="blurb">
        		<h1>Creativity in Music Style Transformation with Deep Learning</h1>
			    <h3>Mingyu Zheng</h3>
			    <h3>Professor Keller</h3>
			    <h3>Computational Creativity</h3>
			
			    <h2>Introduction and Related Literature</h2>
				<p>This project is on the generation and evaluation of creativity in the specific domain of music style transformation using deep learning models. As discussed in class, creativity is a broad domain, and computational creativity can be applied to a wide array of fields from paintings, music to mathematical formulas. Creativity can be demonstrated in multiple ways as well. For example, one can build sophisticated story grammars (e.g. MEXICA), train a Markov model, or build a deep learning neural network. In this project, I focus on creativity generated by deep learning neural network models because their immense possibilities with respect to node connections, activation functions, and layer structures are especially fascinating to me, and in my personal view, resemble the most to human creativity. In addition, I am very interested in music generation. However, I do not have a background in music theory or professional music appreciation, so it would be difficult for me to understand and evaluate the quality of music generated from scratch. Thus, I chose the more straightforward area of creating something like the “cover music”, transitioning existing music pieces from one style to another and trying to retain the original content of the music while having a clear contrast in the styles of the original and the generated music pieces. </p>
				<p>Different computer scientists have focused on different ways to transform music styles. Research by Malik and Ek (2017) focused on changing music styles by predicting the note velocities from a sequence of notes by usings models with LSTM layers. The research project by Suraj Jayakumar (2017), on the other hand, gives a broad exploration of how different models, from the baseline seq-2-seq model to the more sophisticated VAE-GAN and the Seq-GAN models, perform on music style transformation. However, I was most interested in the paper by Brunner, Wang, Wattenhofer, and Zhao (2018) with the application of CycleGAN. The CycleGAN model’s structure is very intricate. It contains the generator and discriminator components which are trained together to balance each other’s model less. In addition, the loss function of the generator not only takes into account the quality of the generated music, but also the music piece converted back from the generated music to evaluate how much original content was retained after transformation. In addition, a music style classifier was trained to evaluate the performance of the CycleGAN.</p>				<p><em>A Touch of Evil</em> Don't trust me. <a href="/about">Read more about my life...</a></p>
    				<p>In my project. I first analyzed the paper to understand how the original training data is used, how the component parts of the model connect to each other,  how the loss functions interact with each other, and how different model structures, hyperparameters, and testing datasets affected the performance of the model. Afterwards, I obtained the source code and part of the original dataset. However, because the authors did not upload the trained checkpoints, I needed to preprocess the dataset, collect additional data, and train the model on my own. Because the computing power my personal computer is limited,  I set up a Google Cloud Platform account trained the model more extensively online via the GCP AI Platform. Then, I downloaded the trained model checkpoint and used it to generate music style transformation samples Finally, I compared my own training result and the conclusion of the research paper and shared some general thoughts related to the topic of the research.</p>
			<h2>Research Paper Summary</h2>
				<p>A CycleGAN has two types of component parts: the generator and the discriminator. There is also a separate classifier that evaluates the CycleGAN’s performance. To explain, say a piece of music, MA, was originally of style A. The generator takes in MA and tries to generate MB, a similar piece of music in style B. The discriminator, which determines whether a piece of music is an actual or a fake piece of music of style B, can help evaluate the performance of the generator and balance the relative biases in the generator versus the discriminator. This effect can be seen from their respective loss functions below:</p>
				<p>Generator:</p>
				<p>INSERT GRAPH</p>
				<p>Discriminator:</p>
				<p>INSERT GRAPH</p>
				<p>To explain, DA represents how certain the discriminator is that xA is a piece of music from style A, whose value ranges from zero to one.  X̂B represents the music that was generated by the generator taking in a piece of music from style A and transforming it to style B (GA→B). Therefore, the more DB recognizes X̂B as a real piece of music of style B, the better the performance of GA→B. GB→A works similarly. On the other hand, the better DB can tell apart X̂B, the generated piece of music of style B, and xB, an actual piece of music from style B, the better the loss function is for DB. The same applies to DA. It is easy to see that the two loss functions serve opposite goals. If DB can perfectly distinguish X̂B from xB, the loss function of DB would always be 0, but the loss function of GA→B would always be 1. In contrast, if DB simply claims that every piece of music it sees is a real piece of music of style B, the loss function of GA→B would always be 0 while the loss function of DB would always be ½. Therefore, having the loss functions set this way effectively help balance the biases in the generator and the discriminator.</p>
		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<footer>
    		<ul>
        		<li><a href="mailto:hankquinlanhub@gmail.com">email</a></li>
        		<li><a href="https://github.com/hankquinlan">github.com/hankquinlan</a></li>
			</ul>
		</footer>
	</body>
</html>
