<!DOCTYPE html>
<html>
	<head>
		<title>Hank Quinlan, Horrible Cop</title>
		<!-- link to main stylesheet -->
		<link rel="stylesheet" type="text/css" href="/css/main.css">
	</head>
	<body>
		<nav>
    		<ul>
        		<li><a href="/">Home</a></li>
	        	<li><a href="/about">About</a></li>
        		<li><a href="/cv">CV</a></li>
        		<li><a href="/blog">Blog</a></li>
    		</ul>
		</nav>
		<div class="container">
    		<div class="blurb">
        		<h1>Creativity in Music Style Transformation with Deep Learning</h1>
			    <h3>Mingyu Zheng</h3>
			    <h3>Professor Keller</h3>
			    <h3>Computational Creativity</h3>
			
			    <h2>Introduction and Related Literature</h2>
				<p>This project is on the generation and evaluation of creativity in the specific domain of music style transformation using deep learning models. As discussed in class, creativity is a broad domain, and computational creativity can be applied to a wide array of fields from paintings, music to mathematical formulas. Creativity can be demonstrated in multiple ways as well. For example, one can build sophisticated story grammars (e.g. MEXICA), train a Markov model, or build a deep learning neural network. In this project, I focus on creativity generated by deep learning neural network models because their immense possibilities with respect to node connections, activation functions, and layer structures are especially fascinating to me, and in my personal view, resemble the most to human creativity. In addition, I am very interested in music generation. However, I do not have a background in music theory or professional music appreciation, so it would be difficult for me to understand and evaluate the quality of music generated from scratch. Thus, I chose the more straightforward area of creating something like the “cover music”, transitioning existing music pieces from one style to another and trying to retain the original content of the music while having a clear contrast in the styles of the original and the generated music pieces. </p>
				<p>Different computer scientists have focused on different ways to transform music styles. Research by Malik and Ek (2017) focused on changing music styles by predicting the note velocities from a sequence of notes by usings models with LSTM layers. The research project by Suraj Jayakumar (2017), on the other hand, gives a broad exploration of how different models, from the baseline seq-2-seq model to the more sophisticated VAE-GAN and the Seq-GAN models, perform on music style transformation. However, I was most interested in the paper by Brunner, Wang, Wattenhofer, and Zhao (2018) with the application of CycleGAN. The CycleGAN model’s structure is very intricate. It contains the generator and discriminator components which are trained together to balance each other’s model less. In addition, the loss function of the generator not only takes into account the quality of the generated music, but also the music piece converted back from the generated music to evaluate how much original content was retained after transformation. In addition, a music style classifier was trained to evaluate the performance of the CycleGAN.</p>				<p><em>A Touch of Evil</em> Don't trust me. <a href="/about">Read more about my life...</a></p>
    				<p>In my project. I first analyzed the paper to understand how the original training data is used, how the component parts of the model connect to each other,  how the loss functions interact with each other, and how different model structures, hyperparameters, and testing datasets affected the performance of the model. Afterwards, I obtained the source code and part of the original dataset. However, because the authors did not upload the trained checkpoints, I needed to preprocess the dataset, collect additional data, and train the model on my own. Because the computing power my personal computer is limited,  I set up a Google Cloud Platform account trained the model more extensively online via the GCP AI Platform. Then, I downloaded the trained model checkpoint and used it to generate music style transformation samples Finally, I compared my own training result and the conclusion of the research paper and shared some general thoughts related to the topic of the research.</p>
			<h2>Research Paper Summary</h2>
				<p>A CycleGAN has two types of component parts: the generator and the discriminator. There is also a separate classifier that evaluates the CycleGAN’s performance. To explain, say a piece of music, MA, was originally of style A. The generator takes in MA and tries to generate MB, a similar piece of music in style B. The discriminator, which determines whether a piece of music is an actual or a fake piece of music of style B, can help evaluate the performance of the generator and balance the relative biases in the generator versus the discriminator. This effect can be seen from their respective loss functions below:</p>
				<p>Generator:</p>
				<p>INSERT GRAPH</p>
				<p>Discriminator:</p>
				<p>INSERT GRAPH</p>
				<p>To explain, DA represents how certain the discriminator is that xA is a piece of music from style A, whose value ranges from zero to one.  X̂B represents the music that was generated by the generator taking in a piece of music from style A and transforming it to style B (GA→B). Therefore, the more DB recognizes X̂B as a real piece of music of style B, the better the performance of GA→B. GB→A works similarly. On the other hand, the better DB can tell apart X̂B, the generated piece of music of style B, and xB, an actual piece of music from style B, the better the loss function is for DB. The same applies to DA. It is easy to see that the two loss functions serve opposite goals. If DB can perfectly distinguish X̂B from xB, the loss function of DB would always be 0, but the loss function of GA→B would always be 1. In contrast, if DB simply claims that every piece of music it sees is a real piece of music of style B, the loss function of GA→B would always be 0 while the loss function of DB would always be ½. Therefore, having the loss functions set this way effectively help balance the biases in the generator and the discriminator.</p>
				<p>The entire CycleGAN structure is as below:</p>
				<p>INSERT GRAPH</p>
				<p>In the above graph, the blue and red arrows denote the domain transfers in the two opposite directions (blue: A to B; red: B to A), and the black arrows point to the loss functions. GA→B and GB→A are two generators which transfer data between A and B. DA and DB are two discriminators which distinguish if the input data is real or fake. DA,m and DB,m are two extra discriminators which force the generators to learn more high-level features. Within this structure, the generator and the discriminator’s architecture graphs and charts are as below:</p>
				<p>Generator:</p>
				<p>INSERT GRAPH</p>
				<p>Discriminator:</p>
				<p>INSERT GRAPH</p>
				<p>The original dataset consists of MIDI music files of Classic, Jazz, and Pop styles. 20 percent of the original dataset is selected to the test dataset while 80 percent is left as the training dataset. After preprocessing, the training dataset consists of 12,341 Jazz, 16,545 Classic and 20,780 Pop samples, where the length of one sample is equal to one phrase, or four bars. To avoid introducing a bias due to the imbalance of genres, the amount of samples in the larger dataset are reduced to match that of the smaller one. For example, when training on Jazz and Classic domains, 12,341 phrases are randomly sampled from the Classic dataset to match the size of the Jazz dataset. It is worth noting that Gaussian noise (N0, σ2D) are introduced to the inputs of the discriminators to further stabilize the GAN training. This improves the robustness and generalization performance of the model.</p>
				<p>For each domain pair and σD value, the research trains three different models: A base model without extra discriminators, a partial model with DA,m and DB,m where m ∈ M = A∪B and a full model with DA,m and DB,m where m ∈ M = A ∪ B ∪ C. These three models are referred to as Mbase, Mpartial and Mfull respectively. This results in a total of 3∗3∗6 = 54 models, from which the best ones are chosen according to the domain transfer strength metric SDtot. I will now explain how SDtot is obtained.</p>
				<p>SDtot is a metric generated from the classifier, which is trained to evaluate style and domain transfer. A style classifier CA,B distinguishes between genre A and B, and it is applied to xA and X̂B in the research. If the style transfer works, CA,B will classify xA as of genre A, and X̂B as of genre B. However, if the style transfer does not work, CA,B might still classify X̂B as of genre A. The more confident the classifier is, the stronger the genre transfer. </p>
				<p>The architecture of the classifier is as below:</p>
				<p>INSERT GRAPH</p>
				<p>When considering a transfer from A to B, CA,B reports the probability PA(x) if the source genre is A, and PB(x) if the source genre is B. A domain transfer from A to B is considered as successful if PA(xA) = CA,B(xA) > 0.5 and PA(X̂B) = CA,B(X̂B) < 0.5. In other words, if the source style is considered to be more likely before the transfer, and less likely after the transfer. According to the genre transfer evaluation metric, the model with σD = 1 performs best (SDtot = 69.7%). The transfer is successful in all cases. The genre classifier assigns high probability to the source genre before the transfer, and low probability after the transfer. However, the metrics for Jazz/Classic and Classic/Pop domains pairs are stronger than that of the Jazz/Pop domain pair. This might be partially due to the fact that the genre classifier itself cannot distinguish Pop and Jazz styles as easily as the other genre pairs even when presented with simply real music data. The below performance metrics of the classifier demonstrates the argument above:</p>
				<p>INSERT GRAPH</p>
			<h2>Additional Experimentation</h2>
				<p>Besides interpreting and analyzing different research paper, I also tried training the model hands-on to see what aspects of creativity can be observed during the training process. Initially, I downloaded the source code on GitHub and part of the training dataset. However, the source code is highly tailored to the local environment and settings of the authors’ own local machines, and no trained checkpoints was given in the GitHub repository. Therefore, I needed to modify and consolidate the code to preprocess the training data. The preprocessing process is shown below:</p>
				<p>INSERT GRAPH</p>
				<p>Besides downloading part of their training dataset, I also downloaded another set of MIDI music files consisted of Classical and Jazz music pieces used in the SeqGan model in Yu, Zhang, Wang, and Yu (2017)’s research. After preprocessing the dataset, I first tried to train the CyclaGAN model locally. However, it was really slow to train each epoch and my laptop got so stuck that I could not do anything else during training. Therefore, I had to cut the training early with merely 20 epochs. With the checkpoint stored after training 20 epochs, I generated testing samples using the primitive model. An interesting thing I observed is that the generated samples barely have any sounds. There is only around two to three 16th nodes in the four-bar sample, while the rest of the sample is just silence. I believe this shows that early on in the training process, the model will focus on training the generator. When the generator produces meaningless music pieces that are clearly distinguished from real jazz music pieces which actually have melodies, the discriminator would have a low loss as there should be no problem to distinguish an almost silent piece of music from music with actual content. However, the generator’s loss function would be a disaster since the generated piece would not be considered as actual jazz music by the discriminator. Therefore, the initial training process should focus on improving the generator.</p>
				<p>To be able to train a more sophisticated model, I decided to train the model online using Google Cloud Platform (GCP)’s AI Platform. I first created a Google Cloud Storage bucket and uploaded the training files. Then, I compiled the model code from a single python module to a Python package, in which I added setup.py and consolidated the model file and the helper files into a “trainer’ directory to allow the model to be called for online training. Afterwards, I trained the model on GCP with part of the original dataset and the additional dataset on Classical and Jazz music for 1000 epochs. I then downloaded the latest checkpoints, rebuilt the model, and generated samples using the model. The resulting generated jazz music pieces sounded much more like real-world music data with actual content, though their quality is still worse than that of the original pieces of classical music. This shows that after 1000 epochs of training, both the generator and the discriminator was able to reduce their loss and have a balanced training result. However, the generator is still unable to keep comprehensive music content of the original piece and there is much room for improvement.</p>
			<h2>Conclusion and Future Work</h2>
				<p>From the research paper and doing training on my own, I learned that CycleGAN is an intricate and effective model for music style transformation. In addition, by collecting the data, consolidating and adding to the code, and packing the CycleGAN model to train on GCP, I understood in much more detail how the models’ functionalities and limitations. Finally, the results I observed in the training process reminded me of the concept of creativity we learned in class. That is, creativity needs restraint. Not any kind of modification on existing pieces of work can be counted as creativity. For example, during the initial training process, although the generator presented me with “new” pieces of music by eliminating almost all content of the original classical music, these generated music pieces should not be regarded as creative because they have no atheatic value. These results also demonstrated the importance of an evaluation component in a creative system, which in this case, is the discriminator built into the CycleGAN model. By incorporating metrics from the discriminator in the generator’s loss function, the generator can “realized” early on in the training process that creating blank music pieces does not serve the purpose of generating music of another style. Consequently, it will adjust itself to produce transformed music pieces that can better trick the discriminator, or from the human perspective, that are of actual aesthetic value as a jazz music piece.</p>
				<p>Because I could not find the trained model checkpoints in the source code, it took me a long time to preprocess the data and try to train the model from scratch. Therefore, I did not have a chance in the timeframe of the project to further tune the hyperparameters or modify the model structures to see if there is any performance. This could be potential work for the future. In addition, I also wanted to try testing the model on rock and jazz music styles. However, I later realized that most rock music pieces I found online had very complicated instrument tracks. Since the preprocessing of the dataset involves merging all music tracks while excluding the drum, the original style and content of these rock music pieces will inevitably be disrupted. With more time in the future, I could potentially find rock music pieces that have less instrument tracks but have a very clear rock style. These music pieces will serve as better training data for the model.</p>
			
		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<footer>
    		<ul>
        		<li><a href="mailto:hankquinlanhub@gmail.com">email</a></li>
        		<li><a href="https://github.com/hankquinlan">github.com/hankquinlan</a></li>
			</ul>
		</footer>
	</body>
</html>
